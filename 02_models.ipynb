{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# module name here\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class encoder_h(nn.Module):\n",
    "    def __init__(self, Y_dim, H_dim, hidden_dim = 16):\n",
    "        super().__init__()\n",
    "        self.make_encoder(Y_dim, H_dim, hidden_dim)\n",
    "  \n",
    "    def make_encoder(self, Y_dim, H_dim, hidden_dim):\n",
    "        self.net = nn.Sequential(nn.Linear(Y_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, hidden_dim), nn.Tanh())\n",
    "        self.mu = nn.Linear(hidden_dim, H_dim)\n",
    "        self.std = nn.Sequential(nn.Linear(hidden_dim, H_dim), nn.ReLU())\n",
    "  \n",
    "    def forward(self, y):\n",
    "        hidden_state = self.net(y)\n",
    "        mu, std = self.mu(hidden_state), self.std(hidden_state)\n",
    "        return dist.Normal(mu, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class encoder_x(nn.Module):\n",
    "    def __init__(self, I_dim, H_dim, X_dim, hidden_size=16, inducing_point_stride=None):\n",
    "        super().__init__()\n",
    "        self.inducing_point_stride = inducing_point_stride\n",
    "        self.hidden_size = hidden_size\n",
    "        self.make_network(I_dim, H_dim, X_dim, hidden_size)\n",
    "\n",
    "    def make_network(self, I_dim, H_dim, X_dim, hidden_size):\n",
    "        self.bilstm = nn.LSTM(input_size=I_dim+H_dim, hidden_size=hidden_size, batch_first=True, bidirectional=True)\n",
    "        # self.densenet = nn.Sequential(nn.Linear(2*hidden_size, X_dim), nn.ReLU())\n",
    "        self.mu = nn.Linear(2*hidden_size, X_dim)\n",
    "        self.sigma = nn.Sequential(nn.Linear(2*hidden_size, X_dim), nn.ReLU())\n",
    "  \n",
    "    def forward(self, i_seq, h_seq):\n",
    "        \"\"\"\n",
    "        i_seq: shape = (BS, T)\n",
    "        h_seq: shape = (BS, T, H_dim)\n",
    "        \"\"\"\n",
    "        BS, T = i_seq.shape\n",
    "        assert(len(i_seq.shape)==2)\n",
    "        lstm_input = torch.cat([i_seq.view(*i_seq.shape, 1), h_seq], dim=-1)\n",
    "        hidden, _ = self.bilstm(lstm_input) #shape(hidden) = (BS, T, 2*hidden_size)\n",
    "    \n",
    "        if self.inducing_point_stride is not None:\n",
    "            mu, sigma = self.mu(hidden[:, ::self.inducing_point_stride]), self.sigma(hidden[:, ::self.inducing_point_stride])\n",
    "        else:\n",
    "            mu, sigma = self.mu(hidden), self.sigma(hidden)\n",
    "        return dist.Normal(mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message passing routines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $l_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def l_n_vectorized(I, X, H):\n",
    "    \"\"\"\n",
    "    I: shape=(BS,S)\n",
    "    X: shape=(BS,S)\n",
    "    H: shape=(BS,S,H_dim)\n",
    "\n",
    "    Output:\n",
    "    out: shape=(BS,S,B,1)\n",
    "    \"\"\"\n",
    "    # sort of a workaround\n",
    "    Z = torch.arange(B).view(*torch.ones(len(X.shape), dtype=int), B).expand(*X.shape, -1)\n",
    "    X = X.view(*X.shape, 1).expand(*X.shape, B)\n",
    "    I = I.view(*I.shape, 1).expand(*I.shape, B)\n",
    "    assert X.shape == I.shape == Z.shape\n",
    "    ll_i = _decoder_i._log_likelihood(I, X, Z) #shape = (BS,S,B)\n",
    "    ll_h = _decoder_h._log_likelihood(H) #shape=(BS,S,B,H_dim)\n",
    "    # print(ll_i.shape, ll_h.shape)\n",
    "    out = (ll_i + ll_h.sum(-1)).unsqueeze(-1) #shape for each = (BS,S,B)\n",
    "    return out #shape for each = (BS,S,B,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\psi_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def psi_n_vectorized(X):\n",
    "    \"\"\"\n",
    "    X: shape=(BS,S)\n",
    "    \"\"\"\n",
    "    init_shape = X.shape\n",
    "    X_mod = _decoder_z.transform_x(X) #shape = (BS,S,B)\n",
    "    X_mod = X_mod.unsqueeze(-2) #shape = (BS,S,1,B) - make 'row vector'\n",
    "\n",
    "    unnorm_logits = _decoder_z.P.view(1,1,*_decoder_z.P.shape) + X_mod #shape = (1,1,B,B) + (BS,S,1,B) = (BS,S,B,B)\n",
    "    normalizer = torch.logsumexp(unnorm_logits, dim=-1, keepdim=True) #shape = (BS,S,B,1)\n",
    "    return (unnorm_logits - normalizer) #shape=(BS,S,B,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $m_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compute_message(psi_matrix, l_vector, prev_message_vector):\n",
    "    \"\"\"\n",
    "    psi_matrix: shape=(..., B,B)\n",
    "    l_vector: shape=(..., B,1)\n",
    "    prev_message_vector: shape=(..., B,1)\n",
    "\n",
    "    Output: \n",
    "    next_message_vector: shape=(..., B,1)\n",
    "    \"\"\"\n",
    "    raw_messages = psi_matrix + l_vector + prev_message_vector #shape=(..., B, B)\n",
    "    next_message_vector = torch.logsumexp(raw_messages, dim=-2, keepdim=True) #shape=(..., 1, B) - 'row vector'\n",
    "    next_message_vector = next_message_vector.transpose(-1,-2)\n",
    "\n",
    "    #Alternate way of doing above two steps in one\n",
    "    # _next_message_vector = torch.logsumexp(raw_messages, dim=-2, keepdim=False).unsqueeze(-1)\n",
    "    # print((next_message_vector - _next_message_vector).sum())\n",
    "\n",
    "    return next_message_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_simulations.ipynb.\n",
      "Converted 02_models.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
