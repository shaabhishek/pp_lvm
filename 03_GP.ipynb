{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp GP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# module name here\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pp_lvm.simulations import TrueParameters, simulate_data, transform_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_jitter_covar(M, eps=1e-6):\n",
    "    identity_matrix = torch.eye(M.shape[-1]).to(M.device)\n",
    "    M = M + eps * identity_matrix\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_covariance_matrix_from_RBFkernel(M, lengthscale=20, eps=1e-6, device=device):\n",
    "    x1 = M.unsqueeze(-2).div(lengthscale)\n",
    "    x2 = M.unsqueeze(-1).div(lengthscale)\n",
    "    K = torch.pow(x1-x2, 2).div(-2).exp()\n",
    "    # Adding jitter for numerical stability\n",
    "    K = add_jitter_covar(K, eps=eps)\n",
    "#     K += 1e-6*torch.eye(M.shape[-1]).to(device)\n",
    "    return K\n",
    "\n",
    "def get_covariance_matrix_from_RBFkernel_new(M1, M2, lengthscale=20, eps=1e-6, device=device):\n",
    "    assert (M1.shape[:-1] == M2.shape[:-1])\n",
    "    x1 = M1.unsqueeze(-1).div(lengthscale)\n",
    "    x2 = M2.unsqueeze(-2).div(lengthscale)\n",
    "    K = torch.pow(x1-x2, 2).div(-2).exp()\n",
    "    # Adding jitter for numerical stability\n",
    "    if (eps > 0) and (M1.shape[-1] == M2.shape[-1]):\n",
    "        K = add_jitter_covar(K, eps=eps)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_predictions(train_x, train_y, test_x, observed_pred, num_plots=1):\n",
    "    \"\"\"\n",
    "    train_x: shape=(BS,S)\n",
    "    train_y: shape=(BS,S)\n",
    "    test_x: shape=(BS,S')\n",
    "    observed_pred: shape=(BS,S')\n",
    "    \"\"\"\n",
    "    train_x = Model_X.reshape_T(train_x, train_x.shape).cpu()\n",
    "    train_y = Model_X.reshape_X(train_y, train_x.shape).cpu()\n",
    "    test_x = Model_X.reshape_T(test_x, test_x.shape).cpu()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for s in range(num_plots):\n",
    "            # Initialize plot\n",
    "            f, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "            # Get upper and lower confidence bounds\n",
    "            lower, upper = observed_pred.confidence_region()\n",
    "            lower = lower.cpu()\n",
    "            upper = upper.cpu()\n",
    "            # Plot training data as black stars\n",
    "            ax.plot(train_x.numpy()[s].flatten(), train_y.detach().numpy()[s], 'k*')\n",
    "            # Plot predictive means as blue line\n",
    "            ax.plot(test_x.numpy()[s].flatten(), observed_pred.mean.cpu().numpy()[s], 'b')\n",
    "\n",
    "            ax.plot(test_x[s].flatten().numpy(), transform_x(test_x[s].flatten()).numpy(), 'r')\n",
    "            # Shade between the lower and upper confidence bounds\n",
    "            ax.fill_between(test_x[s].flatten().numpy(), lower[s].numpy(), upper[s].numpy(), alpha=0.5)\n",
    "            ax.set_xlim([train_x[s].min()-50, train_x[s].max()+50])\n",
    "            ax.set_ylim([-1.2, 1.2])\n",
    "            ax.legend(['Observed Data', 'Mean', 'True', 'Confidence'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Model\n",
    "This is where the mean and kernel functions are setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ExactGPModelLayer(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, BS):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([BS]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([BS]), lengthscale_prior=gpytorch.priors.GammaPrior(250., 1.)),\n",
    "            # gpytorch.kernels.RBFKernel(batch_shape=torch.Size([BS])),\n",
    "            # gpytorch.kernels.RBFKernel(batch_shape=torch.Size([BS]), lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(20, 100)),\n",
    "            batch_shape=torch.Size([BS])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        # import pdb; pdb.set_trace()\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# likelihood = gpytorch.likelihoods.GaussianLikelihood(batch_shape=torch.Size([8]))\n",
    "# model = ExactGPModelLayer(None, None, likelihood, 8)\n",
    "# # model = ExactGPModelLayer(T.view(*T.shape, 1), X, likelihood, 8)\n",
    "\n",
    "# def make_training_inputs(train_inputs):\n",
    "#     return tuple(tri.unsqueeze(-1) if tri.ndimension() == 1 else tri for tri in train_inputs)\n",
    "\n",
    "# def f():\n",
    "# #     model.set_train_data(make_training_inputs((T.view(*T.shape,1),)), X, strict=0)\n",
    "#     model.set_train_data((T.unsqueeze(-1)[:3],), X[:3], strict=0)\n",
    "#     # model()\n",
    "#     import pdb; pdb.set_trace()\n",
    "#     return model(T.view(*T.shape, 1))\n",
    "# f()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Wrapper as an Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Model_X:\n",
    "    def __init__(self, T, X, device=device):\n",
    "        self.Tshape = T.shape\n",
    "        self.Xshape = X.shape\n",
    "        BS = T.shape[0]\n",
    "\n",
    "        train_x = self.reshape_T(T, self.Tshape)\n",
    "        train_y = self.reshape_X(X, self.Tshape)\n",
    "        # initialize likelihood and model\n",
    "        self.likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_prior=gpytorch.priors.NormalPrior(.01,1),\n",
    "                                                                  batch_shape=torch.Size([BS])).to(device)\n",
    "        self.model = ExactGPModelLayer(train_x, train_y, self.likelihood, BS).to(device)\n",
    "\n",
    "    @staticmethod\n",
    "    def reshape_T(T, Tshape):\n",
    "        train_x = T.view(*Tshape, 1)\n",
    "        return train_x\n",
    "\n",
    "    @staticmethod\n",
    "    def reshape_X(X, Tshape):\n",
    "        train_y = X.view(Tshape)\n",
    "        return train_y\n",
    "\n",
    "    def fit(self, T, X, training_iter=1000, lr=0.1, verbose=True):\n",
    "        train_x, train_y = self.reshape_T(T, self.Tshape), self.reshape_X(X, self.Tshape)\n",
    "\n",
    "        self.model.train()\n",
    "        self.likelihood.train()\n",
    "\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': self.model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "        ], lr=lr)\n",
    "\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n",
    "\n",
    "        for i in range(training_iter):\n",
    "            # Zero gradients from previous iteration\n",
    "            optimizer.zero_grad()\n",
    "            # Output from model\n",
    "            output = self.model(train_x)\n",
    "            # Calc loss and backprop gradients\n",
    "            loss = -mll(output, train_y).sum()\n",
    "            loss.backward(retain_graph=True)\n",
    "            if verbose:\n",
    "                if i % (training_iter//10) ==0 :\n",
    "                    print(f'Iter {i + 1}/{training_iter} - Loss: {loss.item()}, \\\n",
    "                    mean lengthscale: {self.model.covar_module.base_kernel.lengthscale.mean().item()}')\n",
    "            optimizer.step()\n",
    "\n",
    "        self.model.eval()\n",
    "        self.likelihood.eval()\n",
    "\n",
    "    def get_posterior(self, test_x):\n",
    "        \"\"\"\n",
    "        test_x: shape: (BS, S, 1), BS: should be the same batch size as the training data or 1 (will broadcast)\n",
    "\n",
    "        Output:\n",
    "\n",
    "        \"\"\"\n",
    "        # Get into evaluation (predictive posterior) mode\n",
    "        self.model.eval()\n",
    "        self.likelihood.eval()\n",
    "\n",
    "        with gpytorch.settings.fast_pred_var():\n",
    "            observed_pred = self.likelihood(self.model(test_x))\n",
    "\n",
    "        return observed_pred\n",
    "\n",
    "    def predict(self, test_x):\n",
    "        \"\"\"\n",
    "        test_x: shape: (BS, S, 1), BS: should be the same batch size as the training data or 1 (will broadcast)\n",
    "        \"\"\"\n",
    "        # Get into evaluation (predictive posterior) mode\n",
    "        self.model.eval()\n",
    "        self.likelihood.eval()\n",
    "\n",
    "        # Test points are regularly spaced along [0,1]\n",
    "        # Make predictions by feeding model through likelihood\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            # test_x = torch.linspace(0, T.max(), 100).view(1,100,1).expand(10,100,1)\n",
    "            observed_pred = self.likelihood(self.model(test_x))\n",
    "\n",
    "        return observed_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def inference_X(T, X, _model_x=None, training_iter=2000, lr=0.1, device=device, verbose=True):\n",
    "    \"\"\"\n",
    "    T: shape=(BS,S)\n",
    "    X: shape=(BS,S)\n",
    "    \"\"\"\n",
    "    if _model_x is None:\n",
    "        _model_x = Model_X(T, X, device=device)\n",
    "        _model_x.fit(T, X, training_iter=training_iter, lr=lr, verbose=verbose)\n",
    "    return _model_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covariance_matrix_from_RBFkernel_new(M1, M2,lengthscale=20, eps=1e-6, device=device):\n",
    "#     print(M1.shape, M2.shape)\n",
    "    assert (M1.shape[:-1] == M2.shape[:-1])\n",
    "    x1 = M1.unsqueeze(-1).div(lengthscale)\n",
    "    x2 = M2.unsqueeze(-2).div(lengthscale)\n",
    "    K = torch.pow(x1-x2, 2).div(-2).exp()\n",
    "    # Adding jitter for numerical stability\n",
    "    if (M1.shape[-1] == M2.shape[-1]):\n",
    "        K = add_jitter_covar(K, eps=eps)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _GP_Exact(train_x, test_x, lengthscale=20, noise=1e-2):\n",
    "    K = get_covariance_matrix_from_RBFkernel_new(train_x, train_x, lengthscale=lengthscale, eps=noise)\n",
    "    L = torch.cholesky(K)\n",
    "#     K_inv = torch.inverse(K)\n",
    "    K_star = get_covariance_matrix_from_RBFkernel_new(train_x, test_x, eps=0)\n",
    "#     F = torch.matmul(K_star.transpose(-1,-2), K_inv)\n",
    "#     F = torch.solve(K_star, K).solution.transpose(-1,-2)\n",
    "    F = torch.cholesky_solve(K_star, L).transpose(-1,-2)\n",
    "    return K,K_star,F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.5367e-07)\n",
      "Tests passed\n"
     ]
    }
   ],
   "source": [
    "def test_res(l=20):\n",
    "    device = 'cpu'\n",
    "    M = torch.rand(2,10).to(device)\n",
    "    \n",
    "    C1 = get_covariance_matrix_from_RBFkernel(M, lengthscale=l, device=device)\n",
    "    \n",
    "    kernel = gpytorch.kernels.RBFKernel().initialize(lengthscale=l).to(device)\n",
    "    C2 = kernel(M.unsqueeze(-1), M.unsqueeze(-1)).evaluate().detach()\n",
    "    \n",
    "    assert (C1.shape==C2.shape)\n",
    "    print(torch.max(torch.abs(C1-C2)))\n",
    "    assert (torch.max(torch.abs(C1-C2)) < 1e-4)\n",
    "    print(\"Tests passed\")\n",
    "    \n",
    "test_res()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _simulate_data_wrapper(S=8, N=400):\n",
    "    TrueParams = TrueParameters(S=S, N=N, device=device)\n",
    "    _,_,T,X,_,_ = simulate_data(TrueParams)\n",
    "    print(f\"Original shapes: T={T.shape}, X={X.shape}\")\n",
    "    return T,X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(T,X):\n",
    "    gp_training_iters = 2000\n",
    "\n",
    "    train_x = T[:,::10]\n",
    "    train_y = X[:,::10]\n",
    "    test_x = T\n",
    "    print(f\"Training data shapes: T={train_x.shape}, X={train_y.shape}\\n\")\n",
    "\n",
    "    posterior_model_x = inference_X(train_x, train_y, training_iter=gp_training_iters, lr=1, device=device, verbose=True)\n",
    "    return posterior_model_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ExactGP:\n",
    "#     def __init__(self, train_x, train_y):\n",
    "#         self.lengthscale\n",
    "    \n",
    "#     def __call__(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# T,X = _simulate_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_analytical(T,X, lengthscale=60, noise=.2):\n",
    "    train_x = T[:,::20]\n",
    "    train_y = X[:,::20]\n",
    "    _, K_star,F = _GP_Exact(train_x, T, lengthscale=lengthscale, noise=noise)\n",
    "    mu = torch.matmul(F,train_y.unsqueeze(-1)).squeeze(-1)\n",
    "    covar_matrix = get_covariance_matrix_from_RBFkernel_new(T, T, eps=1e-6) - torch.matmul(F, K_star)\n",
    "#     covar_matrix = add_jitter_covar(torch.relu(covar_matrix))\n",
    "    return dist.MultivariateNormal(mu, covar_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist_post_gpytorch = train_model(T,X)\n",
    "# dist_post_analytical = train_model_analytical(T, X, 30, .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c=plt.imshow(dist_post_analytical.covariance_matrix[0].cpu())\n",
    "# plt.colorbar(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_predictions_analytical(T[:,::20], X[:,::20], T, dist_post_analytical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def confidence_region(dist_x):\n",
    "#     mean_x, std_x = dist_x.mean, dist_x.stddev\n",
    "# #     std_x = torch.stack([torch.diag(_cov) for _cov in covar_x], dim=0).sqrt()\n",
    "#     return (mean_x - 2*std_x, mean_x + 2*std_x)\n",
    "\n",
    "# def plot_predictions_analytical(train_x, train_y, test_x, dist_posterior):\n",
    "#     s = 0\n",
    "#     f, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "#     # Get upper and lower confidence bounds\n",
    "#     lower, upper = confidence_region(dist_posterior)\n",
    "#     lower = lower.cpu()\n",
    "#     upper = upper.cpu()\n",
    "\n",
    "#     # Plot training data as black stars\n",
    "#     ax.plot(train_x.cpu().numpy()[s].flatten(), train_y.cpu().detach().numpy()[s], 'k*')\n",
    "#     # Plot predictive means as blue line\n",
    "#     ax.plot(T.cpu().numpy()[s].flatten(), dist_posterior.mean.cpu().numpy()[s], 'b')\n",
    "\n",
    "#     ax.plot(T[s].cpu().flatten().numpy(), transform_x(T[s].cpu().flatten()).numpy(), 'r')\n",
    "#     #             # Shade between the lower and upper confidence bounds\n",
    "#     ax.fill_between(T[s].cpu().flatten().numpy(), lower[s].numpy(), upper[s].numpy(), alpha=0.5)\n",
    "#     #             ax.set_xlim([train_x[s].min()-50, train_x[s].max()+50])\n",
    "#     ax.set_ylim([-1.2, 1.2])\n",
    "#     ax.legend(['Observed Data', 'Mean', 'True', 'Confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posterior_model_x = train_model(T,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed_predictions = posterior_model_x.predict(T.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot_predictions(train_x, train_y, test_x, observed_predictions, num_plots=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_simulations.ipynb.\n",
      "Converted 02_models.ipynb.\n",
      "Converted 03_GP.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
